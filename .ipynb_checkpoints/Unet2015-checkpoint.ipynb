{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELVVAbdCVrGl"
   },
   "source": [
    "# TFDS Dataset Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tLP6lf9eRbAA",
    "outputId": "23c8eaab-2677-475a-de39-2b0949bf8f05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uninstalling tensorflow-datasets-4.0.1:\n",
      "  Would remove:\n",
      "    /usr/local/bin/tfds\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_datasets-4.0.1.dist-info/*\n",
      "    /usr/local/lib/python3.6/dist-packages/tensorflow_datasets/*\n",
      "Proceed (y/n)? y\n",
      "  Successfully uninstalled tensorflow-datasets-4.0.1\n",
      "Collecting git+git://github.com/jingjing-shi/datasets\n",
      "  Cloning git://github.com/jingjing-shi/datasets to /tmp/pip-req-build-krvvx95h\n",
      "  Running command git clone -q git://github.com/jingjing-shi/datasets /tmp/pip-req-build-krvvx95h\n",
      "Requirement already satisfied: absl-py in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.10.0)\n",
      "Requirement already satisfied: attrs>=18.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (20.2.0)\n",
      "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.3.3)\n",
      "Requirement already satisfied: dm-tree in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.1.5)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.16.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (1.18.5)\n",
      "Requirement already satisfied: promise in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (3.12.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (2.23.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (1.15.0)\n",
      "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.24.0)\n",
      "Requirement already satisfied: termcolor in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (1.1.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (4.41.1)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (1.12.1)\n",
      "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (0.7)\n",
      "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.6/dist-packages (from tensorflow-datasets===3.2.1-nightly) (3.3.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-datasets===3.2.1-nightly) (50.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets===3.2.1-nightly) (2020.6.20)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets===3.2.1-nightly) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets===3.2.1-nightly) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->tensorflow-datasets===3.2.1-nightly) (1.24.3)\n",
      "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-metadata->tensorflow-datasets===3.2.1-nightly) (1.52.0)\n",
      "Requirement already satisfied: zipp>=0.4; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from importlib_resources->tensorflow-datasets===3.2.1-nightly) (3.4.0)\n",
      "Building wheels for collected packages: tensorflow-datasets\n",
      "  Building wheel for tensorflow-datasets (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for tensorflow-datasets: filename=tensorflow_datasets-3.2.1_nightly-cp36-none-any.whl size=3495334 sha256=5368939526038ea3b86cf48bf72b5816f32d6aef964645a49bebac5a3aec5e9e\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-byl14ph3/wheels/b1/bf/25/112bd40633bdeb00a184d30650219d44d9523630a3c242c614\n",
      "Successfully built tensorflow-datasets\n",
      "Installing collected packages: tensorflow-datasets\n",
      "Successfully installed tensorflow-datasets-3.2.1-nightly\n"
     ]
    }
   ],
   "source": [
    "!pip3 uninstall tensorflow_datasets\n",
    "!pip3 install git+git://github.com/jingjing-shi/datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "Z-MqUHkLFqQ4"
   },
   "outputs": [],
   "source": [
    "import pydicom as dicom\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nibabel as nib\n",
    "import math\n",
    "import numpy as np\n",
    "from tensorflow.data.experimental import sample_from_datasets\n",
    "from tensorflow.image import central_crop\n",
    "import skimage.transform as trans\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import Model\n",
    "from skimage.util import crop\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras import initializers\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import metrics\n",
    "import SimpleITK as sitk\n",
    "from tensorflow.keras import models\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from datetime import datetime\n",
    "from packaging import version\n",
    "from utilities import *\n",
    "from unet_model_file import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZcHpE8M0Rli1",
    "outputId": "90347e81-f32a-4fe7-947d-9a9432588152"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:`interleave_parallel_reads` argument of `tfds.ReadConfig` is deprecated and will be removed in a future version. Please use `interleave_cycle_length` instead.\n",
      "WARNING:absl:`interleave_parallel_reads` argument of `tfds.ReadConfig` is deprecated and will be removed in a future version. Please use `interleave_cycle_length` instead.\n"
     ]
    }
   ],
   "source": [
    "## I wrote a tfds dataset for BRATS2015, so we can load tfds data\n",
    "data_dir = 'gs://duke-tfds'\n",
    "# load both HGG and LGG. 80/20 train val split\n",
    "# Init dataset\n",
    "\n",
    "train_data, info = tfds.load('Brats2015',data_dir=data_dir,with_info=True, split='train[:80%]', \n",
    "                             read_config=tfds.ReadConfig(interleave_parallel_reads=tf.data.experimental.AUTOTUNE,interleave_block_length=8), shuffle_files=True)\n",
    "val_data, info = tfds.load('Brats2015',data_dir=data_dir,with_info=True, split='train[20%:]', \n",
    "                           read_config=tfds.ReadConfig(interleave_parallel_reads=tf.data.experimental.AUTOTUNE, interleave_block_length=8), shuffle_files=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KVYoyYwYrHHR"
   },
   "source": [
    "# Pre-process data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AgzQNaZIf6DL"
   },
   "outputs": [],
   "source": [
    "# Create training data [BATCH BEFORE MAPPING]\n",
    "processed_train_data = train_data.map(lambda x: binarize(x),num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "processed_train_data = processed_train_data.map(lambda x: cast(x),num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "processed_train_data = processed_train_data.map(lambda x: standardize(x), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "processed_train_data = processed_train_data.map(lambda x: reshaping(x), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "#Create validation data\n",
    "processed_val_data = val_data.map(lambda x: binarize(x), num_parallel_calls = tf.data.experimental.AUTOTUNE) \n",
    "processed_val_data = processed_val_data.map(lambda x: cast(x), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "processed_val_data = processed_val_data.map(lambda x: standardize(x), num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "processed_val_data = processed_val_data.map(lambda x: reshaping(x), num_parallel_calls = tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 318
    },
    "id": "YZ_6LVX-Ux3-",
    "outputId": "ab9d26f0-1ada-471e-8554-af3a1f1aaa6b"
   },
   "outputs": [],
   "source": [
    "# Examine sample data\n",
    "ds_iter = iter(processed_train_data)\n",
    "sample = next(ds_iter)\n",
    "image = sample[0][0]\n",
    "mask = sample[1][0]\n",
    "fig, axes = plt.subplots(1,2, figsize=(10,10))\n",
    "axes[0].imshow(tf.squeeze(image))\n",
    "axes[0].set_title('Image')\n",
    "axes[1].imshow(tf.squeeze(mask))\n",
    "axes[1].set_title('Mask')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create model and train on training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "Pe1Enm_StG9A"
   },
   "outputs": [],
   "source": [
    "unet = unet_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "kbkTTMgtueKp",
    "outputId": "387515fd-19df-4ef1-a673-56c4c15f64cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Unet\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, 240, 240, 1) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 240, 240, 32) 320         input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 240, 240, 32) 9248        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 120, 120, 32) 0           conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 120, 120, 64) 18496       max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 120, 120, 64) 36928       conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 60, 60, 64)   0           conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 60, 60, 128)  73856       max_pooling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_29 (Conv2D)              (None, 60, 60, 128)  147584      conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 30, 30, 128)  0           conv2d_29[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_30 (Conv2D)              (None, 30, 30, 256)  295168      max_pooling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_31 (Conv2D)              (None, 30, 30, 256)  590080      conv2d_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 15, 15, 256)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_32 (Conv2D)              (None, 15, 15, 512)  1180160     max_pooling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_33 (Conv2D)              (None, 15, 15, 512)  2359808     conv2d_32[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 15, 15, 512)  0           conv2d_33[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_4 (UpSampling2D)  (None, 30, 30, 512)  0           dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 30, 30, 256)  0           conv2d_31[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_34 (Conv2D)              (None, 30, 30, 256)  524544      up_sampling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 30, 30, 512)  0           dropout_2[0][0]                  \n",
      "                                                                 conv2d_34[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_35 (Conv2D)              (None, 30, 30, 256)  1179904     concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_36 (Conv2D)              (None, 30, 30, 256)  590080      conv2d_35[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_5 (UpSampling2D)  (None, 60, 60, 256)  0           conv2d_36[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_37 (Conv2D)              (None, 60, 60, 128)  131200      up_sampling2d_5[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 60, 60, 256)  0           conv2d_29[0][0]                  \n",
      "                                                                 conv2d_37[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_38 (Conv2D)              (None, 60, 60, 128)  295040      concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_39 (Conv2D)              (None, 60, 60, 128)  147584      conv2d_38[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_6 (UpSampling2D)  (None, 120, 120, 128 0           conv2d_39[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_40 (Conv2D)              (None, 120, 120, 64) 32832       up_sampling2d_6[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 120, 120, 128 0           conv2d_27[0][0]                  \n",
      "                                                                 conv2d_40[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_41 (Conv2D)              (None, 120, 120, 64) 73792       concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_42 (Conv2D)              (None, 120, 120, 64) 36928       conv2d_41[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_7 (UpSampling2D)  (None, 240, 240, 64) 0           conv2d_42[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_43 (Conv2D)              (None, 240, 240, 32) 8224        up_sampling2d_7[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_7 (Concatenate)     (None, 240, 240, 64) 0           conv2d_25[0][0]                  \n",
      "                                                                 conv2d_43[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_44 (Conv2D)              (None, 240, 240, 32) 18464       concatenate_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_45 (Conv2D)              (None, 240, 240, 32) 9248        conv2d_44[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_46 (Conv2D)              (None, 240, 240, 2)  578         conv2d_45[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_47 (Conv2D)              (None, 240, 240, 1)  3           conv2d_46[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 7,760,069\n",
      "Trainable params: 7,760,069\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "unet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lvRhMiyFV6FY"
   },
   "outputs": [],
   "source": [
    "# Create a TensorBoard callback\n",
    "log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5, verbose=0, mode='auto', \n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uEn01j_u6z1"
   },
   "outputs": [],
   "source": [
    "# Fit on 2015 brats data\n",
    "unet.fit(processed_train_data, epochs = 20, verbose =1, validation_data = processed_val_data,callbacks=[tensorboard_callback, stopping,checkpoint])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Unet2015.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
